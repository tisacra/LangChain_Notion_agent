# === LangChain + Notion ãƒ–ãƒ­ãƒƒã‚¯è¿½è¨˜ç‰ˆ (ConversationBufferMemory + VectorStoreçµ±åˆ) ===
'''
graph TD
    A[ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›] --> B{ä¿å­˜æŒ‡ç¤ºã‚ã‚Šã‹}
    B -->|ã‚ã‚Š| C[ConversationBufferMemory ã‹ã‚‰å±¥æ­´å–å¾—]
    C --> D[å±¥æ­´ã‚’è¦ç´„ #40;LLM#41;]
    D --> E[Notion å›ºå®šãƒšãƒ¼ã‚¸ã«ãƒ–ãƒ­ãƒƒã‚¯è¿½è¨˜]
    D --> F[è¦ç´„å†…å®¹ã‚’ VectorStore ã«è¿½åŠ ]
    B -->|ãªã—| G[VectorStore ã§é¡ä¼¼å±¥æ­´æ¤œç´¢]
    G --> H[ConversationBufferMemory ã®å±¥æ­´ã¨æ¤œç´¢çµæœã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æŒ¿å…¥]
    H --> I[LLM å¿œç­”ç”Ÿæˆ]
    I --> J[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å›ç­”è¡¨ç¤º]
    J --> K[ConversationBufferMemory ã«ç™ºè©±è¿½åŠ 
            +
            è³ªå•ãƒ»å›ç­”ãƒšã‚¢ã‚’ VectorStore ã«è¿½åŠ ]
    K --> A
'''

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
import os
from dotenv import load_dotenv
import pickle
import shutil

import subsystem.Notion_func as Notion_func

SUMMARIZE_LOCAL = False
VALID_SAVE_LOCAL = False

# === å‹•çš„è¦ç´„è¨­å®š ===
from langchain_ollama import ChatOllama

# è¦ç´„å°‚ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
local_summarizer = ChatOllama(model="gemma3:4b", verbose=True)

turn_counter = 0
SUMMARY_INTERVAL = 3  # 3ã‚¿ãƒ¼ãƒ³ã”ã¨ã«è¦ç´„
summary_memory = ""  # è¦ç´„è“„ç©ç”¨

# === ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ===
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ORGANIZATION_ID = os.getenv("OPENAI_ORGANIZATION_ID")
DATABASE_ID = os.getenv("DATABASE_ID")
PAGE_ID = os.getenv("PAGE_ID")

MEMORY_PATH = "memory/"
VECTORSTORE_PATH = "vectorstore/"

if not PAGE_ID:
    raise ValueError("PAGE_ID is not set in .env file")

if not DATABASE_ID:
    raise ValueError("DATABASE_ID is not set in .env file")

# === LLMã®åˆæœŸåŒ– ===
llm = ChatOpenAI(
    api_key=OPENAI_API_KEY,
    organization=OPENAI_ORGANIZATION_ID,
    model_name="gpt-4o-mini"
)

embeddings = OpenAIEmbeddings()


# === Memoryã®åˆæœŸåŒ– ===
def load_memory():
    if os.path.exists(MEMORY_PATH + PAGE_ID + ".pkl"):
        with open(MEMORY_PATH + PAGE_ID + ".pkl", "rb") as f:
            return pickle.load(f)
    return ConversationBufferMemory(return_messages=True)

def save_memory(memory):
    with open(MEMORY_PATH + PAGE_ID + ".pkl", "wb") as f:
        pickle.dump(memory, f)

def refresh_memory():
    """ä¼šè©±å±¥æ­´ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    global memory
    memory = ConversationBufferMemory(return_messages=True)
    if os.path.exists(MEMORY_PATH + PAGE_ID + ".pkl"):
        os.remove(MEMORY_PATH + PAGE_ID + ".pkl")
    print("ğŸ”„ ä¼šè©±å±¥æ­´ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ")

memory = load_memory()

# === VectorStoreã®åˆæœŸåŒ– ===
def load_vectorstore():
    """VectorStoreã‚’ãƒ­ãƒ¼ãƒ‰ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆ"""
    if os.path.exists(VECTORSTORE_PATH + PAGE_ID):
        return FAISS.load_local(VECTORSTORE_PATH + PAGE_ID, embeddings, allow_dangerous_deserialization=True)
    
    # åˆæœŸåŒ–æ™‚ã¯æœ€ä½1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦
    vectorstore = FAISS.from_texts(
        ["åˆæœŸåŒ–ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¤œç´¢ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚"],
        embeddings
    )
    vectorstore.save_local(VECTORSTORE_PATH + PAGE_ID)
    return vectorstore

def save_vectorstore(vectorstore):
    """VectorStoreã‚’ä¿å­˜"""
    vectorstore.save_local(VECTORSTORE_PATH + PAGE_ID)

def refresh_vectorstore():
    """VectorStoreã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    global vectorstore
    # åˆæœŸåŒ–æ™‚ã¯æœ€ä½1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦
    vectorstore = FAISS.from_texts(
        ["åˆæœŸåŒ–ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¤œç´¢ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚"],
        embeddings
    )
    if os.path.exists(VECTORSTORE_PATH):
        try:
            shutil.rmtree(VECTORSTORE_PATH)
        except Exception as e:
            print(f"âš ï¸ VectorStoreã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return
    print("ğŸ”„ VectorStoreã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ")

def refresh_all():
    """ä¼šè©±å±¥æ­´ã¨VectorStoreä¸¡æ–¹ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    refresh_memory()
    refresh_vectorstore()
    print("âœ¨ å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ")

vectorstore = load_vectorstore()


# === ä¿å­˜æŒ‡ç¤ºåˆ¤å®š ===
def is_valid_save_command(user_input):
    prompt = f"""
    ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã¯ã€AIã¨ã®è­°è«–å†…å®¹ã‚’Notionã«ä¿å­˜ã™ã‚‹ã‚ˆã†ã«ä¾é ¼ã—ã¦ã„ã‚‹æ„å›³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
    ã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã ã‘ã§ç­”ãˆã¦ãã ã•ã„ã€‚

    ç™ºè©±: "{user_input}"
    """
    if not VALID_SAVE_LOCAL:
        decision = llm.invoke(prompt).content
    else:
        decision = local_summarizer.invoke(prompt).content
    #print(f"åˆ¤æ–­çµæœ: {decision}")
    return "ã¯ã„" in decision

def is_new_topic(input):
    prompt = f"""
    ä»¥ä¸‹ã®ã‚„ã‚Šå–ã‚Šã®æµã‚Œã¯ã€ã€Œç•°ãªã‚‹æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã®é–‹å§‹ã€ã ã¨æ€ã‚ã‚Œã¾ã™ã‹ï¼Ÿ
    ã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã ã‘ã§ç­”ãˆã¦ãã ã•ã„ã€‚

    ç™ºè©±: "{input}"
    """
    decision = llm.invoke(prompt).content
    print(f"åˆ¤æ–­çµæœ: {decision}")
    return "ã¯ã„" in decision

# === è¦ç´„é–¢æ•° ===
def summarize_memory():
    global summary_memory
    current_history = "\n".join([msg.content for msg in memory.load_memory_variables({})["history"]])
    prompt = f"""
    ä»¥ä¸‹ã®è­°è«–å±¥æ­´ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚
    ç‰¹ã«ã€è³ªç–‘å¿œç­”ã‚’é‡è¦–ã—ã¦ã€[è³ªå•]â†’[å›ç­”]ã®å½¢å¼ã§ç¤ºã™ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

    {current_history}
    """
    if not SUMMARIZE_LOCAL:
        summary = llm.invoke(prompt).content
    else:
        summary = local_summarizer.invoke(prompt).content
    summary_memory += f"\n{summary}"
    print(f"ğŸ“ è¦ç´„è¿½åŠ : {summary}")
    memory.chat_memory.clear()
    print(memory.load_memory_variables({}))
    return summary

def save_summary():
    print("ğŸ’¾ ä¿å­˜æŒ‡ç¤ºãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æŒ‡å®šãƒšãƒ¼ã‚¸ã«è¿½è¨˜ã—ã¾ã™ã€‚")
    # Notionä¿å­˜
    Notion_func.append_to_page(PAGE_ID, summarize_memory())

def input_flow(user_input):
    global turn_counter
    
    if user_input is None or user_input == "":
        return ""    
    
    if user_input.lower() in ["refresh", "clear"]:
        refresh_all()
        return "refresh"
    elif user_input.lower() == "refresh memory":
        refresh_memory()
        return "refresh memory"
    elif user_input.lower() == "refresh vectorstore":
        refresh_vectorstore()
        return "refresh vectorstore"
    
    if is_valid_save_command(user_input):
        save_summary()
        try:
            save_vectorstore(vectorstore)
        except Exception as e:
            print(f"âš ï¸ VectorStoreã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    else:
        # é¡ä¼¼å±¥æ­´æ¤œç´¢
        docs = vectorstore.similarity_search(user_input, k=2)
        retrieved = "\n".join([d.page_content for d in docs])

        # Memoryå±¥æ­´å–å¾—
        history = "".join([msg.content for msg in memory.load_memory_variables({})["history"]])

        # LLMå¿œç­”ç”Ÿæˆ
        prompt = f"éå»ã®è­°è«–: {retrieved}\nç›´è¿‘ã®ä¼šè©±: {history}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}"

        # æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã®æ¤œå‡º
        if is_new_topic(prompt):
            print("ğŸ”„ æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã®é–‹å§‹ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚ä¼šè©±å±¥æ­´ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚")
            refresh_memory()

        result = llm.invoke(prompt).content
        print(result)
        # é€šå¸¸ç™ºè©±ã‚‚ãƒšã‚¢ã§ä¿å­˜
        pair_text = f"[è³ªå•] {user_input}\n[å›ç­”] {result}"
        vectorstore.add_texts([pair_text])
        save_vectorstore(vectorstore)

        # Memoryã«ã‚‚è¿½åŠ 
        memory.chat_memory.add_user_message(user_input)
        memory.chat_memory.add_ai_message(result)
        turn_counter += 1
    
        # å‹•çš„è¦ç´„
        if turn_counter % SUMMARY_INTERVAL == 0:
            summarize_memory()
            print(f"ğŸ”„ å‹•çš„è¦ç´„: {summary_memory}")


# === å®Ÿè¡Œä¾‹ ===
if __name__ == "__main__":
    turn_counter = 0
    try:
        while True:
            user_input = input("> ")
            if user_input.lower() in ["exit", "quit"]:
                break
            input_flow(user_input)
            
    finally:
        save_memory(memory)
        print("âœ… ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
