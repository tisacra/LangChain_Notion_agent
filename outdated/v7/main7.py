# === LangChain + Notion ãƒ–ãƒ­ãƒƒã‚¯è¿½è¨˜ç‰ˆ (ConversationBufferMemory + VectorStoreçµ±åˆ) ===

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
import os
from dotenv import load_dotenv
import pickle
import shutil

import subsystem.Notion_func as Notion_func


DEBUG = True

SUMMARIZE_LOCAL = False
VALID_SAVE_LOCAL = False
VALID_NEW_TOPIC_LOCAL = False

if DEBUG:
    print("Debug mode is enabled.")

# === å‹•çš„è¦ç´„è¨­å®š ===
from langchain_ollama import ChatOllama

# è¦ç´„å°‚ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
local_summarizer = ChatOllama(model="gemma3:4b", verbose=True)

turn_counter = 0
SUMMARY_INTERVAL = 3  # 3ã‚¿ãƒ¼ãƒ³ã”ã¨ã«è¦ç´„
summary_memory = ""  # è¦ç´„è“„ç©ç”¨

# === ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ===
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ORGANIZATION_ID = os.getenv("OPENAI_ORGANIZATION_ID")
DATABASE_ID = os.getenv("DATABASE_ID")
PAGE_ID = os.getenv("PAGE_ID")

MEMORY_PATH = "memory/"
VECTORSTORE_PATH = "vectorstore/"

if not PAGE_ID:
    raise ValueError("PAGE_ID is not set in .env file")

if not DATABASE_ID:
    raise ValueError("DATABASE_ID is not set in .env file")

# === LLMã®åˆæœŸåŒ– ===
llm = ChatOpenAI(
    api_key=OPENAI_API_KEY,
    organization=OPENAI_ORGANIZATION_ID,
    model_name="gpt-4o-mini"
)

embeddings = OpenAIEmbeddings()

# === ãƒšãƒ¼ã‚¸IDæ›´æ–°é–¢æ•° ===
def update_page_id(new_page_id):
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªPAGE_IDã‚’æ›´æ–°ã™ã‚‹é–¢æ•°"""
    global PAGE_ID
    PAGE_ID = new_page_id

def get_page_id():
    return PAGE_ID

# === Memoryã®åˆæœŸåŒ– ===

def load_memory():
    if os.path.exists(MEMORY_PATH + PAGE_ID + ".pkl"):
        with open(MEMORY_PATH + PAGE_ID + ".pkl", "rb") as f:
            if DEBUG:
                print(f"ğŸ’¾ ä¼šè©±å±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ : {PAGE_ID}")
            return pickle.load(f)
    return ConversationBufferMemory(return_messages=True)

def save_memory():
    global memory
    with open(MEMORY_PATH + PAGE_ID + ".pkl", "wb") as f:
        if DEBUG:
            print(f"ğŸ’¾ ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ : {PAGE_ID}")
        pickle.dump(memory, f)

def refresh_memory():
    """ä¼šè©±å±¥æ­´ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    global memory
    memory = ConversationBufferMemory(return_messages=True)
    if os.path.exists(MEMORY_PATH + PAGE_ID + ".pkl"):
        os.remove(MEMORY_PATH + PAGE_ID + ".pkl")
    if DEBUG:
        print(f"ğŸ”„ ä¼šè©±å±¥æ­´ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ : {PAGE_ID}")

memory = load_memory()

# === VectorStoreã®åˆæœŸåŒ– ===
def load_vectorstore():
    """VectorStoreã‚’ãƒ­ãƒ¼ãƒ‰ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆ"""
    if os.path.exists(VECTORSTORE_PATH + PAGE_ID):
        if DEBUG:
            print(f"ğŸ’¾ VectorStoreã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ : {PAGE_ID}")
        return FAISS.load_local(VECTORSTORE_PATH + PAGE_ID, embeddings, allow_dangerous_deserialization=True)
    
    # åˆæœŸåŒ–æ™‚ã¯æœ€ä½1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦
    vectorstore = FAISS.from_texts(
        ["åˆæœŸåŒ–ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¤œç´¢ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚"],
        embeddings
    )
    vectorstore.save_local(VECTORSTORE_PATH + PAGE_ID)
    if DEBUG:
        print(f"ğŸ’¾ VectorStoreã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ : {PAGE_ID}")
    return vectorstore

def save_vectorstore():
    vectorstore.save_local(VECTORSTORE_PATH + PAGE_ID)
    if DEBUG:
        print(f"ğŸ’¾ VectorStoreã‚’ä¿å­˜ã—ã¾ã—ãŸ : {PAGE_ID}")

def refresh_vectorstore():
    """VectorStoreã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    global vectorstore
    # åˆæœŸåŒ–æ™‚ã¯æœ€ä½1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦
    vectorstore = FAISS.from_texts(
        ["åˆæœŸåŒ–ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¤œç´¢ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚"],
        embeddings
    )
    if os.path.exists(VECTORSTORE_PATH):
        try:
            shutil.rmtree(VECTORSTORE_PATH)
        except Exception as e:
            print(f"âš ï¸ VectorStoreã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return
    if DEBUG:
        print(f"ğŸ”„ VectorStoreã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ : {PAGE_ID}")

def refresh_all():
    """ä¼šè©±å±¥æ­´ã¨VectorStoreä¸¡æ–¹ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    refresh_memory()
    refresh_vectorstore()
    print("âœ¨ å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ : {PAGE_ID}")

vectorstore = load_vectorstore()


# === ä¿å­˜æŒ‡ç¤ºåˆ¤å®š ===
def is_valid_save_command(user_input):
    prompt = f"""
    ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã¯ã€AIã¨ã®è­°è«–å†…å®¹ã‚’Notionã«ä¿å­˜ã™ã‚‹ã‚ˆã†ã«ä¾é ¼ã—ã¦ã„ã‚‹æ„å›³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
    ã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã ã‘ã§ç­”ãˆã¦ãã ã•ã„ã€‚

    ç™ºè©±: "{user_input}"
    """
    if not VALID_SAVE_LOCAL:
        decision = llm.invoke(prompt).content
    else:
        decision = local_summarizer.invoke(prompt).content
    #print(f"åˆ¤æ–­çµæœ: {decision}")
    return "ã¯ã„" in decision

def is_new_topic(input):
    prompt = f"""
    ä»¥ä¸‹ã®ã‚„ã‚Šå–ã‚Šã®æµã‚Œã¯ã€æ˜ã‚‰ã‹ã«ã€Œç•°ãªã‚‹æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã®é–‹å§‹ã€ã ã¨æ€ã‚ã‚Œã¾ã™ã‹ï¼Ÿ
    ç™ºè©±: "{input}"

    è©±é¡Œã®å¤‰æ›´ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€Œã¯ã„ã€ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    ç›¸æ§Œã®å ´åˆã¯ã€Œã„ã„ãˆã€ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    æ¬¡ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§åˆ¤æ–­çµæœã¨ç†ç”±ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    
    åˆ¤æ–­çµæœï¼šã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€
    ç†ç”±ï¼šæœ€é•·200æ–‡å­—ç¨‹åº¦
    """
    if VALID_NEW_TOPIC_LOCAL:
        decision = local_summarizer.invoke(prompt).content
    else:
        decision = llm.invoke(prompt).content
    decision_part = decision.split("åˆ¤æ–­çµæœï¼š")[1].split("ç†ç”±ï¼š")[0].strip()
    if DEBUG:
        print(decision)
        print(f"åˆ¤æ–­çµæœ: {decision_part}")
    return "ã¯ã„" in decision

# === è¦ç´„ã‚µãƒ–ãƒ«ãƒ¼ãƒãƒ³ ===
def summarize_memory():
    global summary_memory
    current_history = "\n".join([msg.content for msg in memory.load_memory_variables({})["history"]])
    prompt = f"""
    ä»¥ä¸‹ã®è­°è«–å±¥æ­´ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚
    ç‰¹ã«ã€è³ªç–‘å¿œç­”ã‚’é‡è¦–ã—ã¦ã€[è³ªå•]â†’[å›ç­”]ã®å½¢å¼ã§ç¤ºã™ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

    {current_history}
    """
    if not SUMMARIZE_LOCAL:
        summary = llm.invoke(prompt).content
    else:
        summary = local_summarizer.invoke(prompt).content
    summary_memory += f"\n{summary}"
    
    if DEBUG:
        print("prompt : ", prompt)
        print(f"ğŸ“ è¦ç´„è¿½åŠ : {summary}")
    memory.chat_memory.clear()
    memory.chat_memory.add_message(summary)
    return summary

def save_summary():
    print("ğŸ’¾ ä¿å­˜æŒ‡ç¤ºãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æŒ‡å®šãƒšãƒ¼ã‚¸ã«è¿½è¨˜ã—ã¾ã™ã€‚")
    # Notionä¿å­˜
    Notion_func.append_to_page(PAGE_ID, summarize_memory())
    memory.chat_memory.clear()

def input_flow(user_input):
    global turn_counter
    
    if user_input is None or user_input == "":
        return ""    
    
    if user_input.lower() in ["refresh", "clear"]:
        refresh_all()
        return "refresh"
    elif user_input.lower() == "refresh memory":
        refresh_memory()
        return "refresh memory"
    elif user_input.lower() == "refresh vectorstore":
        refresh_vectorstore()
        return "refresh vectorstore"
    
    if is_valid_save_command(user_input):
        save_summary()
        try:
            save_vectorstore()
        except Exception as e:
            print(f"âš ï¸ VectorStoreã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        # é¡ä¼¼å±¥æ­´æ¤œç´¢
        docs = vectorstore.similarity_search(user_input, k=2)
        retrieved = "\n".join([d.page_content for d in docs])

        # Memoryå±¥æ­´å–å¾—
        history = "".join([msg.content for msg in memory.load_memory_variables({})["history"]])

        # LLMå¿œç­”ç”Ÿæˆ
        prompt = f"éå»ã®è­°è«–: {retrieved}\nç›´è¿‘ã®ä¼šè©±: {history}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}"

        # æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã®æ¤œå‡º
        if is_new_topic(prompt):
            print("ğŸ”„ æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã®é–‹å§‹ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚ä¼šè©±å±¥æ­´ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚")
            refresh_memory()

        result = llm.invoke(prompt).content
        #print(result)
        # é€šå¸¸ç™ºè©±ã‚‚ãƒšã‚¢ã§ä¿å­˜
        pair_text = f"[è³ªå•] {user_input}\n[å›ç­”] {result}"
        vectorstore.add_texts([pair_text])
        save_vectorstore()

        # Memoryã«ã‚‚è¿½åŠ 
        memory.chat_memory.add_user_message(user_input)
        memory.chat_memory.add_ai_message(result)
        turn_counter += 1
        if DEBUG:
            print("Memoryå±¥æ­´:", memory.load_memory_variables({})["history"])

    
        # å‹•çš„è¦ç´„
        if turn_counter % SUMMARY_INTERVAL == 0:
            summarize_memory()
            print(f"ğŸ”„ å‹•çš„è¦ç´„: {summary_memory}")
        
        return result


# === å®Ÿè¡Œä¾‹ ===
if __name__ == "__main__":
    turn_counter = 0
    try:
        while True:
            user_input = input("> ")
            if user_input.lower() in ["exit", "quit"]:
                break
            print(input_flow(user_input))
            

    finally:
        save_memory(memory)
        print("âœ… ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
