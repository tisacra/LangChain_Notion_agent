# === LangChain + Notion ãƒ–ãƒ­ãƒƒã‚¯è¿½è¨˜ç‰ˆ (ConversationBufferMemory æ°¸ç¶šåŒ–å¯¾å¿œ) ===

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from notion_client import Client
import os
from dotenv import load_dotenv
import pickle

# === ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ===
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ORGANIZATION_ID = os.getenv("OPENAI_ORGANIZATION_ID")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
PAGE_ID = os.getenv("PAGE_ID")
MEMORY_PATH = "memory.pkl"

if not PAGE_ID:
    raise ValueError("PAGE_ID is not set in .env file")

# === Notionã¨LLMã®åˆæœŸåŒ– ===
notion = Client(auth=NOTION_TOKEN)
llm = ChatOpenAI(
    api_key=OPENAI_API_KEY,
    organization=OPENAI_ORGANIZATION_ID,
    model_name="gpt-3.5-turbo"
)

# === Memoryã®åˆæœŸåŒ– ===
def load_memory():
    if os.path.exists(MEMORY_PATH):
        with open(MEMORY_PATH, "rb") as f:
            return pickle.load(f)
    return ConversationBufferMemory(return_messages=True)

def save_memory(memory):
    with open(MEMORY_PATH, "wb") as f:
        pickle.dump(memory, f)

memory = load_memory()

# === ãƒ–ãƒ­ãƒƒã‚¯è¿½è¨˜é–¢æ•° ===
def append_to_page(page_id, content):
    notion.blocks.children.append(
        block_id=page_id,
        children=[
            {
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {
                    "rich_text": [
                        {"type": "text", "text": {"content": content}}
                    ]
                }
            }
        ]
    )
    print(f"ğŸ“ ãƒšãƒ¼ã‚¸ã«è¿½è¨˜ã—ã¾ã—ãŸ: {content}")

# === ä¿å­˜æŒ‡ç¤ºåˆ¤å®š ===
def is_valid_save_command(user_input):
    if "ä¿å­˜" not in user_input:
        return False

    prompt = f"""
    ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã¯ã€AIã¨ã®è­°è«–å†…å®¹ã‚’Notionã«ä¿å­˜ã™ã‚‹ã‚ˆã†ã«ä¾é ¼ã—ã¦ã„ã‚‹æ„å›³ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
    ã€Œã¯ã„ã€ã¾ãŸã¯ã€Œã„ã„ãˆã€ã§ç­”ãˆã¦ãã ã•ã„ã€‚

    ç™ºè©±: "{user_input}"
    """
    decision = llm.invoke(prompt).content
    print(f"åˆ¤æ–­çµæœ: {decision}")
    return "ã¯ã„" in decision

# === å®Ÿè¡Œä¾‹ ===
if __name__ == "__main__":
    try:
        while True:
            user_input = input("> ")
            if user_input.lower() in ["exit", "quit"]:
                break

            memory.chat_memory.add_user_message(user_input)

            if is_valid_save_command(user_input):
                print("ğŸ’¾ ä¿å­˜æŒ‡ç¤ºãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æŒ‡å®šãƒšãƒ¼ã‚¸ã«è¿½è¨˜ã—ã¾ã™ã€‚")
                summary_prompt = "ã“ã‚Œã¾ã§ã®è­°è«–å†…å®¹ã‚’Notionã«ä¿å­˜ã™ã‚‹ã®ã«é©ã—ãŸå½¢ã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚"
                messages = memory.load_memory_variables({})["history"] + f"\n{summary_prompt}"
                content = llm.invoke(messages).content
                append_to_page(PAGE_ID, content)
                memory.chat_memory.add_ai_message(content)
            else:
                messages = memory.load_memory_variables({})["history"] + f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}"
                result = llm.invoke(messages).content
                print(result)
                memory.chat_memory.add_ai_message(result)

    finally:
        save_memory(memory)
        print("ğŸ“ ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
