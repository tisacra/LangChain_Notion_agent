# LangGraph ãƒãƒ¼ãƒ‰å®Ÿè£…ï¼šæ–°ã—ã„ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆæ§‹æˆã«å¯¾å¿œ

from langchain.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
from typing import TypedDict, List
import os

from urllib3 import response



DEBUG = True
if DEBUG:
    print("Debug mode is enabled.")

CHAT_LOCAL = False
ABSTRACTION_LOCAL = False
SUMMARIZE_LOCAL = False
VALID_SAVE_LOCAL = False

USE_LOCAL_LLM = CHAT_LOCAL or ABSTRACTION_LOCAL or SUMMARIZE_LOCAL or VALID_SAVE_LOCAL
if USE_LOCAL_LLM:
    print("Local LLM is enabled.")
    # === å‹•çš„è¦ç´„è¨­å®š ===
    from langchain_ollama import ChatOllama

    # è¦ç´„å°‚ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    local_summarizer = ChatOllama(model="gemma3:4b", verbose=True)

# === ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ===
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_ORGANIZATION_ID = os.getenv("OPENAI_ORGANIZATION_ID")
DATABASE_ID = os.getenv("DATABASE_ID")
PAGE_ID = os.getenv("PAGE_ID")
VECTORSTORE_PATH = "vectorstore/"

# === LLMã®åˆæœŸåŒ– ===
llm = ChatOpenAI(
    api_key=OPENAI_API_KEY,
    organization=OPENAI_ORGANIZATION_ID,
    model_name="gpt-4o-mini"
)


SUMMARY_INTERVAL = 3  # 3ã‚¿ãƒ¼ãƒ³ã”ã¨ã«è¦ç´„

# ---- State å‹å®šç¾© ----

class GraphState(TypedDict):
    history: List[dict]  # rawå±¥æ­´
    augmented_query: str
    specific_docs: List[Document]
    association_docs: List[Document]
    response: str
    summary: str
    page_id: str
    save_flag: bool
    
# === å‹•çš„è¦ç´„è¨­å®š ===
embeddings = OpenAIEmbeddings()

# ---- åˆæœŸåŒ– ----
def load_vectorstore():
    """VectorStoreã‚’ãƒ­ãƒ¼ãƒ‰ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆ"""
    if os.path.exists(VECTORSTORE_PATH + PAGE_ID):
        if DEBUG:
            print(f"ğŸ’¾ VectorStoreã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ : {PAGE_ID}")
        return FAISS.load_local(VECTORSTORE_PATH + PAGE_ID, embeddings, allow_dangerous_deserialization=True)
    
    # åˆæœŸåŒ–æ™‚ã¯æœ€ä½1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦
    vectorstore = FAISS.from_texts(
        ["åˆæœŸåŒ–ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¤œç´¢ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚"],
        embeddings
    )
    vectorstore.save_local(VECTORSTORE_PATH + PAGE_ID)
    if DEBUG:
        print(f"ğŸ’¾ VectorStoreã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ : {PAGE_ID}")
    return vectorstore

vectorstore = load_vectorstore()

# ---- ãƒãƒ¼ãƒ‰å®Ÿè£… ----

def augment_query_node(state: GraphState) -> GraphState:
    user_input = state["history"][-1]["content"]
    context = " ".join([h["content"] for h in state["history"][-3:-1]])
    state["augmented_query"] = f"{context} {user_input}"
    return state


def specific_node(state: GraphState) -> GraphState:
    results = vectorstore.similarity_search(state["augmented_query"], k=3, filter={"type": "specific"})
    state["specific_docs"] = results
    return state


def association_node(state: GraphState) -> GraphState:
    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½è±¡åŒ–
    prompt = f"ä»¥ä¸‹ã®ã‚„ã‚Šå–ã‚Šã‚’æŠ½è±¡åŒ–ã—ã¦ãã ã•ã„ï¼š\n{state['augmented_query']}"

    if ABSTRACTION_LOCAL:
        response = local_summarizer.invoke(prompt).content
    else:
        response = llm.invoke(prompt).content
    
    abstract_query = f"[ABSTRACT] {response}"
    results = vectorstore.similarity_search(abstract_query, k=3, filter={"type": "abstract"})
    state["association_docs"] = results
    return state


def generate_response_node(state: GraphState) -> GraphState:
    specific = [doc.page_content for doc in state["specific_docs"]]
    association = [doc.page_content for doc in state["association_docs"]]
    context = "\n".join(["å…·ä½“çš„æ–‡è„ˆ : " + specific, "æŠ½è±¡çš„æ–‡è„ˆ" + association])
    user_input = state["history"][-1]["content"]
    response = ""
    if CHAT_LOCAL:
        response = local_summarizer.invoke(f"{user_input}\nå‚è€ƒ: {context}").content
    else:
        response = llm.invoke(f"{user_input}\nå‚è€ƒ: {context}").content
    state["response"] = f"{response}"
    return state


def register_pair_node(state: GraphState) -> GraphState:
    pair = f"Q: {state['history'][-1]['content']}\nA: {state['response']}"
    vectorstore.add_documents([
        Document(page_content=f"[ABSTRACT] {pair}", metadata={"type": "abstract"})
    ])
    return state


def update_history_node(state: GraphState) -> GraphState:
    state["history"].append({"role": "assistant", "content": state["response"]})
    return state


def summarize_history_node(state: GraphState) -> GraphState:
    if len(state["history"]) <= 1:
        state["summary"] = "[SUMMARY]ï¼ˆå±¥æ­´ãŒçŸ­ã„ãŸã‚è¦ç´„ãªã—ï¼‰"
    else:
        full_text = "\n".join([f"{h['role']}: {h['content']}" for h in state["history"]])
        state["summary"] = f"[SUMMARY] {full_text[:1000]}..."
    return state


def save_trigger_node(state: GraphState) -> GraphState:
    # æ¡ä»¶åˆ†å²ã®ã¿è¡Œã†ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã€å®Ÿå‡¦ç†ãªã—
    if state["history"][-1]["content"] == "save":
        state["save_flug"] = True
    else:
        state["save_flug"] = False
    return state


def save_node(state: GraphState) -> GraphState:
    # Notionä¿å­˜å‡¦ç†ï¼ˆçœç•¥ï¼‰
    summary_doc = Document(
        page_content=state["summary"],
        metadata={"type": "summary", "page_id": state.get("page_id", "unknown")}
    )
    vectorstore.add_documents([summary_doc])
    state["history"] = []  # å±¥æ­´ãƒªã‚»ãƒƒãƒˆ
    return state
