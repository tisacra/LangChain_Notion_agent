# LangGraph ãƒãƒ¼ãƒ‰å®Ÿè£…ï¼šæ–°ã—ã„ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆæ§‹æˆã«å¯¾å¿œ
from langchain.memory import ConversationBufferMemory
import os
import pickle

from langchain.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.documents import Document
from langgraph.types import Command
from typing import TypedDict, List, Annotated
import getpass
import os
from dotenv import load_dotenv
import sys




def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


# ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ - çµ¶å¯¾ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from subsystem import Notion_func


DEBUG = True
if DEBUG:
    print("Debug mode is enabled.")

SUMMARIZE_LENGTH = 6  # è¦ç´„ã®é–¾å€¤

CHAT_LOCAL = False
ABSTRACTION_LOCAL = False
SUMMARIZE_LOCAL = False
VALID_SAVE_LOCAL = False

USE_LOCAL_LLM = CHAT_LOCAL or ABSTRACTION_LOCAL or SUMMARIZE_LOCAL or VALID_SAVE_LOCAL
if USE_LOCAL_LLM:
    print("Local LLM is enabled.")
    # === å‹•çš„è¦ç´„è¨­å®š ===
    from langchain_ollama import ChatOllama

    # è¦ç´„å°‚ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    local_summarizer = ChatOllama(model="gemma3:4b", verbose=True)

# === ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ ===
load_dotenv()

_set_env("OPENAI_API_KEY")
_set_env("OPENAI_ORGANIZATION_ID")

# === LLMã®åˆæœŸåŒ– ===
llm = ChatOpenAI(
    model_name="gpt-4o-mini"
)


SUMMARY_INTERVAL = 3  # 3ã‚¿ãƒ¼ãƒ³ã”ã¨ã«è¦ç´„

MEMORY_PATH = "./memory/"
VECTORSTORE_PATH = "./vectorstore/"

# ---- State å‹å®šç¾© ----

from typing_extensions import Annotated
import operator


class message(TypedDict):
    role: str  # "user" or "assistant"
    content: str

class state(TypedDict):
    history: List[message]
    augmented_query: str = ""
    specific_docs: Annotated[List[Document], operator.add]  # è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰ã®æ›´æ–°ã‚’è¨±å¯
    association_docs: Annotated[List[Document], operator.add]  # è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰ã®æ›´æ–°ã‚’è¨±å¯
    input: dict = {}
    response: str = ""
    summary: str = ""
    user_id: str = ""
    page_id: str = ""
    save_flag: bool = False
    vectorstore: FAISS = None

def const_state(history : List[message] = [], 
                augmented_query: str = "", 
                specific_docs: List[Document] = [], 
                association_docs: List[Document] = [], 
                input: dict = {},
                response: str = "", 
                summary: str = "", 
                user_id: str = "", 
                page_id: str = "", 
                save_flag: bool = False,
                vectorstore: FAISS = None) -> state:
    return {
        "history": history,
        "augmented_query": augmented_query,
        "specific_docs": specific_docs,
        "association_docs": association_docs,
        "input": input,
        "response": response,
        "summary": summary,
        "user_id": user_id,
        "page_id": page_id,
        "save_flag": save_flag,
        "vectorstore": vectorstore
    }

class InitiateOutput(TypedDict):
    user_id: str
    history: List[message]
    vectorstore: FAISS

embeddings = OpenAIEmbeddings()

# ---- ãƒãƒ¼ãƒ‰å®Ÿè£… ----

def load_memory_node(state: state) -> Annotated[InitiateOutput, ("user_id", "history", "vectorstore")]:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—
    user_id = input("ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ=default) : ")
    if not user_id:
        user_id = "default"
    
    # memory
    m_path = MEMORY_PATH + user_id + ".pkl"
    if os.path.exists(m_path):
        with open(m_path, "rb") as f:
            if DEBUG:
                print(f"ğŸ’¾ ä¼šè©±å±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ : {user_id}")
            memory = pickle.load(f)
    else:
        print(f"ã¯ã˜ã‚ã¾ã—ã¦ {user_id}ã•ã‚“ï¼")
        memory = ConversationBufferMemory(return_messages=True)
    context = memory.load_memory_variables({})["history"]
    if DEBUG:
        print("ğŸ’¬ ä¼šè©±å±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ : ", context)
    
    # vectorstore
    vs_path = VECTORSTORE_PATH + "/" + user_id
    """VectorStoreã‚’ãƒ­ãƒ¼ãƒ‰ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆ"""
    if os.path.exists(vs_path):
        if DEBUG:
            print(f"ğŸ’¾ VectorStoreã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ : {vs_path}")
        vectorstore = FAISS.load_local(vs_path, embeddings, allow_dangerous_deserialization=True)
    else:
        # åˆæœŸåŒ–æ™‚ã¯æœ€ä½1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦
        vectorstore = FAISS.from_texts(
            ["åˆæœŸåŒ–ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¤œç´¢ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚"],
            embeddings
        )
        vectorstore.save_local(vs_path)
        if DEBUG:
            print(f"ğŸ’¾ VectorStoreã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ : {vs_path}")
    context.append(state["input"])
    return {
        "user_id" : user_id, "history" : context, "vectorstore" : vectorstore}


def augment_query_node(state: state) -> Annotated[str, "augmented_query"]:
    print("ğŸ’¬ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª : ", state["history"])
    if len(state["history"]) == 1:
        context =  state["history"][0]["role"] + ": " + state["history"][0]["content"]
        print("test1")
    elif len(state["history"]) <= SUMMARIZE_LENGTH:
        context = "\n".join([h["role"] + ": " + h["content"] for h in state["history"][-(len(state)-1):-1]])
        print("test2")
    else:
        context = "\n".join([h["role"] + ": " + h["content"] for h in state["history"][-(SUMMARIZE_LENGTH+1):-1]])
        print("test3")
    print("ğŸ’¬ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ : ", context)
    return {"augmented_query" : context}


def specific_node(state: state) -> Annotated[List[Document], "specific_docs"]:
    print("ğŸ’¬ å…·ä½“çš„æ–‡è„ˆ : ", state["augmented_query"])
    return {"specific_docs" : state["vectorstore"].similarity_search(state["augmented_query"], k=3, filter={"type": "specific"})}


def association_node(state: state) -> Annotated[List[Document], "association_docs"]:
    prompt = f"ä»¥ä¸‹ã®ã‚„ã‚Šå–ã‚Šã‚’æŠ½è±¡åŒ–ã—ã¦ãã ã•ã„ï¼š{state['augmented_query']}"

    if ABSTRACTION_LOCAL:
        response = local_summarizer.invoke(prompt).content
    else:
        response = llm.invoke(prompt).content
    print("ğŸ’¬ æŠ½è±¡åŒ– : ", response)
    abstract_query = f"[ABSTRACT] {response}"
    return {"association_docs" : state["vectorstore"].similarity_search(abstract_query, k=3, filter={"type": "abstract"})}


def generate_response_node(state: state) -> Annotated[str, "response"]:
    history = "\n".join([f"{h['role']}: {h['content']}" for h in state["history"]])
    specific = "\n".join([doc.page_content for doc in state["specific_docs"]])
    association = "\n".join([doc.page_content for doc in state["association_docs"]])
    if DEBUG:
        print("ğŸ’¬ å±¥æ­´ : ", history)
        print("ğŸ’¬ é–¢é€£ã™ã‚‹å…·ä½“çš„æ–‡è„ˆ : ", specific)
        print("ğŸ’¬ é–¢é€£ã™ã‚‹æŠ½è±¡çš„æ–‡è„ˆ : ", association)
    context = "å±¥æ­´ : " + history + "\n"+ "é–¢é€£ã™ã‚‹å…·ä½“çš„æ–‡è„ˆ : " + specific + "\n" + "é¡ä¼¼ã™ã‚‹æŠ½è±¡çš„æ–‡è„ˆ" + association
    user_input = state["input"]["content"]
    if CHAT_LOCAL:
        return local_summarizer.invoke(f"{user_input}\nå‚è€ƒ: {context}").content
    else:
        return {"response" : llm.invoke(f"{user_input}\nå‚è€ƒ: {context}").content}


def register_pair_node(state: state) -> None:
    pair = f"Q: {state['history'][-1]['content']}\nA: {state['response']}"
    state["vectorstore"].add_documents([
        Document(page_content=f"[ABSTRACT] {pair}", metadata={"type": "abstract"})
    ])
    state["vectorstore"].add_documents([
        Document(page_content=f"[SPECIFIC] {pair}", metadata={"type": "specific"})
    ])


def update_history_node(state: state) -> Annotated[List[dict], "history"]:
    history_old = state["history"].copy()
    print("ğŸ’¬ å±¥æ­´ : ", history_old)
    update = [{"role": "assistant", "content": state["response"]}]
    updated = history_old + update
    if DEBUG:
        print("ğŸ’¬ å±¥æ­´æ›´æ–° : ", updated)
    return {"history" : updated}


def summarize_history_node(state: state) -> dict:
    if len(state["history"]) <= SUMMARIZE_LENGTH:
        summary =  ""
    else:
        full_text = "\n".join([f"{h['role']}: {h['content']}" for h in state["history"]])
        prompt = f"ä»¥ä¸‹ã®è­°è«–å±¥æ­´ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n{full_text}"
        if SUMMARIZE_LOCAL:
            summary = local_summarizer.invoke(prompt).content
        else:
            summary = llm.invoke(prompt).content
        return {"history" : [{"role" : "summary", "content" : summary}],"summary" : summary}


def note_trigger_node(state: state) -> Annotated[bool, "save_flag"]:
    print(state)
    return {"save_flag" : state["input"]["content"] == "save"}


def note_node(state: state) -> None:
    summary_doc = Document(
        page_content=state["summary"],
        metadata={"type": "summary", "page_id": state["page_id"]}
    )
    state["vectorstore"].add_documents([summary_doc])
    print("ğŸ’¾ ä¿å­˜æŒ‡ç¤ºãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æŒ‡å®šãƒšãƒ¼ã‚¸ã«è¿½è¨˜ã—ã¾ã™ã€‚")
    pages = Notion_func.get_pages()
    print("ğŸ’¾ ä¿å­˜å…ˆãƒšãƒ¼ã‚¸å€™è£œä¸€è¦§\n")
    for page_id, page_name in pages:
        print(f"No.{pages.index((page_id, page_name)) + 1} ãƒšãƒ¼ã‚¸å: {page_name}, ID: {page_id}")
    print("\nğŸ’¾ ä¿å­˜å…ˆãƒšãƒ¼ã‚¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚-> ")
    while True:
        try:
            page_number = int(input())
            if 1 <= page_number <= len(pages):
                break
            else:
                print("ğŸ’¾ ç„¡åŠ¹ãªç•ªå·ã§ã™ã€‚å†åº¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except ValueError:
            print("ğŸ’¾ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    page_id = pages[page_number - 1][0]
    prompt = f"""
ä»¥ä¸‹ã®è­°è«–å±¥æ­´ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«ã€è³ªç–‘å¿œç­”ã‚’é‡è¦–ã—ã¦ã€[æ–‡è„ˆ]ã€[è³ªå•]â†’[å›ç­”]ã®å½¢å¼ã§ç¤ºã™ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

{"\n".join([f"{h['role']}: {h['content']}\n" for h in state["history"]])}
"""
    summary = (
        local_summarizer.invoke(prompt).content
        if SUMMARIZE_LOCAL else llm.invoke(prompt).content
    )
    Notion_func.append_to_page(page_id, summary)

def save_node(state: state) -> None:
    print("ğŸ’¾ è­°è«–å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã™ã€‚")
    memory = ConversationBufferMemory(return_messages=True)
    for msg in state["history"]:
        memory.chat_memory.add_message(msg)
    m_path = MEMORY_PATH + state["user_id"] + ".pkl"
    with open(m_path, "wb") as f:
        if DEBUG:
            print(f"ğŸ’¾ ä¼šè©±å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸ : {state["user_id"]}")
        pickle.dump(memory, f)